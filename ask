#!/usr/bin/env python
import math
import doc_parser
import nltk
import tree_parser
import ask_modules
import ask_parse
import stanford_utils
import ginger_python2 as grammar_checker

import sys
ner_tagger = stanford_utils.new_NERtagger()
why_keywords = ["because"]
def contains_reason(sent):
    for why_keyword in why_keywords:
            if why_keyword in sent:
                return True
    return False

def contains_time(ner_sent, pos_sent):
    ps = ['during', 'since', 'when', 'before', 'after']
    for i in xrange(0, len(pos_sent)):
        tup = pos_sent[i]
        if tup[1] == "IN":
            if i + 1 < len(ner_sent) and \
                    (ner_sent[i+1][1] == "DATE" or ner_sent[i+1][1] == "TIME"):
                return True
        if tup[0] in ps:
            return True
    return False

def contains_loc(ner_sent, pos_sent):
    ps = ['where']
    for i in xrange(0, len(pos_sent)):
        tup = pos_sent[i]
        if tup[1] == "IN":
            if i + 1 < len(ner_sent) and \
                    (ner_sent[i+1][1] == "LOCATION" or ner_sent[i+1][1] == "ORGANIZATION"):
                return True
        if tup[0] in ps:
            return True
    return False

def contains_name(tagged_sent):
    for tup in tagged_sent:
        if tup[1] == "PERSON":
            return True
        elif tup[0].lower() == "he" or tup[0].lower() == "she":
            return True
    return False

def contains_quant(sent, tagged):
    # tokens = nltk.tokenize.word_tokenize(sent)
    # for i in xrange(0, len(tokens)):
    #     if str.isdigit(str(tokens[i])):
    #         if i + 1 < len(tokens) and tagged_sent[i+1][1].endswith('s'):
    #             return True
    # return False
    for i in xrange(len(tagged)):
        if tagged[i][1] == "CD":
            for j in xrange(len(tagged[i:])):
                if tagged[i+j][1] == "NNS" or tagged[i+j][1] == "NN":
                    return True
    return False

def preprocess_sents(sents):
    preds = []
    for sent in sents:
        tree = tree_parser.sent_to_tree(sent)
    if tree_parser.contains_appos(tree):
        preds += tree_parser.appps_to_sents(tree)
    else:
        pred = tree_parser.sent_to_predicate(tree)
        preds.append(pred)
    return preds


def main(wiki_path, n):
    title, sents = doc_parser.doc_to_sents(wiki_path)
    questions = []

    sents = [sent for sent in sents if 10 <= sent.count(" ") <= 30]
    sents = sents[:3*n]

    for sent in sents:
        # bonus for average len
        score = (20 - math.fabs(sent.count(" ")-10))*0.5

        # bonus for question difficulties

        # tokenize into words for each sentence
        sent_tokens = nltk.word_tokenize(sent)
        # tag for words in each sentence
        pos_sent = nltk.pos_tag(sent_tokens)
        ner_sent = ner_tagger.tag(sent_tokens)
        # binary question
        binary_q, pos_binary, ner_binary = ask_modules.get_binary(sent, pos_sent[:], ner_sent[:], twist=False)
        # pos_binary = nltk.pos_tag(binary_q_tokens)
        # ner_binary = ner_tagger.tag(binary_q_tokens)
        parsed_sent = tree_parser.sent_to_tree(sent)
        # deductions for errors
        binary_q2, errs = grammar_checker.correct_sent(binary_q.capitalize()+"?")
        questions.append((binary_q.capitalize()+"?", score-errs+2))

        # bonus for more pps
        pps = len(tree_parser.get_phrases(parsed_sent, "PP", reversed=False, sort=False))
        score += pps-1

        # when
        if contains_time(ner_binary[:], pos_binary[:]):
            question = ask_modules.get_when(sent, pos_binary[:], ner_binary[:])
            # correct grammar and find errors
            question2, errs = grammar_checker.correct_sent(question)
            # deductions for errors
            if question.count(" ") > 5:
                questions.append((question, score-errs+4))
        # where
        if contains_loc(ner_binary, pos_binary):
            question = ask_modules.get_where(sent, pos_binary[:], ner_binary[:]).capitalize()
            # correct grammar and find errors
            question2, errs = grammar_checker.correct_sent(question)
            # deductions for errors
            if question.count(" ") > 5:
                questions.append((question, score-errs+4))

        # who/what
        # if contains_name(ner_binary):
        #     question = ask_modules.get_who(tagged_sent).capitalize()
        #     # correct grammar and find errors
        #     question, errs = grammar_checker.correct_sent(question)
        #     # deductions for errors
        #     if question.count(" ") > 5:
        #         questions.append((question, score-errs+3))
        # else:
        #     question = ask_modules.get_what(tagged_sent).capitalize()
        #     # correct grammar and find errors
        #     question, errs = grammar_checker.correct_sent(question)
        #     # deductions for errors
        #     if question.count(" ") > 5:
        #         questions.append((question, score-errs+2))

        # who/what
        if contains_name(ner_binary):
            question = ask_modules.get_who(pos_sent).capitalize()
            # correct grammar and find errors
            question, errs = grammar_checker.correct_sent(question)
            # deductions for errors
            if question.count(" ") > 5:
                questions.append((question, score-errs+5))
        else:
            question = ask_modules.get_what(pos_binary).capitalize()
            question, errs = grammar_checker.correct_sent(question)
            if question.count(" ") > 5:
                questions.append((question, score-errs+5))
        #why
        if contains_reason(sent):
            question = ask_modules.get_why(sent_tokens[:], pos_binary[:]).capitalize()
            # correct grammar and find errors
            question, errs = grammar_checker.correct_sent(question)
            # deductions for errors
            if question.count(" ") > 5:
                questions.append((question, score-errs+5))

        # how-many
        elif contains_quant(sent, pos_sent[:]):
            question = ask_modules.get_howmany(sent_tokens[:], pos_sent[:]).capitalize()
            # correct grammar and find errors
            question, errs = grammar_checker.correct_sent(question)
            # deductions for errors
            if question.count(" ") > 5:
                questions.append((question, score-errs+5))

    # while not out_queue.empty():
    #     questions.append(out_queue.get())
    # print "questions size " + str(len(questions))

    ranked_questions = sorted(questions, key=lambda x:(-x[1],x[0]))[:n]
    for question in ranked_questions:
        sys.stdout.write(question[0]+" "+"\n")

import time
# start = time.time()
for i in xrange(1, 9):
    start = time.time()
    if i == 4:
        continue
    print i
    wiki_path = "test/a"+str(i)+".htm"
    main(wiki_path, 10)
    print time.time() - start
# main("test_sw/a1.htm", 10)
# print time.time() - start
# main(sys.argv[1], int(sys.argv[2]))
