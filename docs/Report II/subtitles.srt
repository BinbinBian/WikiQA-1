100:00:00,000 --> 00:00:05,066Hi everyone, we are team ByteMe. This is our second progress report.200:00:05,067 --> 00:00:10,832In the previous work, we finished building all question modules separately with syntax rules.300:00:10,833 --> 00:00:16,899The modules include binary question module and who/what/when/where/why question modules.400:00:16,900 --> 00:00:19,999There are several problems in the previous work.500:00:20,000 --> 00:00:24,132First, we didn’t consider to generate questions with appositions.600:00:24,133 --> 00:00:27,899Now we fixed this problem by parsing sentences.700:00:27,900 --> 00:00:33,732Second, we didn’t distinguish the case when NP represents name of person and name of an object.800:00:33,733 --> 00:00:38,999We fixed this problem by using Human Name tagger and Stanford NER tagger.900:00:39,000 --> 00:00:47,999Now we work in two teams, one is responsible to build pipeline for question generation and the other is responsible for answer extraction.1000:00:48,000 --> 00:00:56,999In the question system, we token the article into sentences first, and then we generate questions by pass each sentence to different question modules.1100:00:57,000 --> 00:01:03,832We have successfully generate questions, but the rate of correctness is pretty low, especially for long sentences.1200:01:03,833 --> 00:01:11,132So in the next step, we need an evaluator to determine which question module is suitable for each sentence.1300:01:11,133 --> 00:01:12,999How to evaluate?1400:01:13,000 --> 00:01:22,766Maybe we can check syntax correctness or give some combinations with high ranking scores, such as location with where module and name with who module.1500:01:22,767 --> 00:01:33,532In the answer system, we build a word vector based on the words showed in the question, and then use this word vector to check the similarity of each sentence in the article.1600:01:33,533 --> 00:01:39,532After selecting the best matching sentence, we extract answer based on syntax rules.1700:01:39,533 --> 00:01:46,366We have the right sentence in most of time, however, some answers contain unnecessary information.1800:01:46,367 --> 00:01:52,599We need to get a more concise and fluent answer by extracting parse subtree for each sentence.1900:01:52,600 --> 00:01:54,999Here are details for our future plan.2000:01:55,000 --> 00:01:57,433Thanks for watching.